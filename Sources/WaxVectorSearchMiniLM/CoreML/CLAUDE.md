# Sources/WaxVectorSearchMiniLM/CoreML Module

## Purpose
Provides on-device text embeddings using the all-MiniLM-L6-v2 transformer model via CoreML. This is the default embedding provider for Wax's text-based RAG pipeline.

## Key Types

| Type | Role |
|------|------|
| `MiniLMEmbeddings` | Class. Loads the CoreML model + BERT tokenizer, produces 384-dimensional embeddings. |
| `BertTokenizer` | Struct/Class. WordPiece tokenizer for BERT-family models. Handles tokenization, padding, attention masks. |
| `BatchInputBuffers` | Struct. Reusable buffers for batched tokenization to avoid repeated allocation. |
| `all_MiniLM_L6_v2` | CoreML auto-generated model class. Wraps the `.mlmodelc` bundle. |

## Architecture

- **Model loading**: Uses a `ModelCache` (NSLock-protected) to avoid redundant CoreML compilations. Key is `(computeUnits, allowLowPrecisionAccumulationOnGPU)`.
- **Default compute units**: `.cpuAndNeuralEngine` (ANE optimized for transformer attention, avoids GPU dispatch overhead).
- **Input**: Tokenized text with sequence length bucketing (32, 64, 128, 256, 384, 512) for efficient batching.
- **Output**: 384-dimensional Float32 embeddings decoded from MLMultiArray.

## Embedding Decoding

`decodeEmbeddings` handles multiple MLMultiArray layouts:
- 2D `[batch, dim]` (most common)
- 3D `[batch, 1, dim]` or `[batch, dim, 1]`
- 1D `[dim]` for single inputs
- Fallback: flat buffer divided by batch size

Supports both `Float32` and `Float16` data types. Uses `vDSP.convertElements` (Accelerate) for fast Float16->Float32 conversion.

## Dimensions

- Input sequence length: up to 512 tokens
- Output embedding: 384 dimensions
- L2-normalization is performed by callers (not internally)

## Testing Support

- `@_spi(Testing)` exposes `_decodeEmbeddingsForTesting` for unit testing embedding decode paths.
- `Overrides` struct allows injecting custom model URLs and tokenizer factories for test isolation.
- `Overrides.missingModel` and `Overrides.missingTokenizer` simulate error paths.

## Dependencies

- `CoreML`, `Accelerate`, `Foundation`
- Bundle resource: `all-MiniLM-L6-v2.mlmodelc` (compiled CoreML model)

## Platform Requirements

- `@available(macOS 15.0, iOS 18.0, *)` due to CoreML API requirements.

## Common Pitfalls

- CoreML/Espresso can deadlock on concurrent model loads; the `ModelCache` serializes loads under a lock.
- Sequence length bucketing is required for batch efficiency; the tokenizer pads to the nearest bucket size.
- The model outputs `var_554` (auto-generated name from ONNX conversion); this is the sentence embedding tensor.


<claude-mem-context>
# Recent Activity

<!-- This section is auto-generated by claude-mem. Edit content outside the tags. -->

*No recent activity*
</claude-mem-context>